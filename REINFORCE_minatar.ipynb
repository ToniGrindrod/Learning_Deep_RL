{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAtUO4yiu-Dm",
        "outputId": "e9d9a652-3ea4-4f8b-91e7-2599e4bd5931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Cannot import matplotlib and/or seaborn.Will not be able to render the environment.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "import neptune\n",
        "from matplotlib import pyplot as plt\n",
        "from minatar import Environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install minatar"
      ],
      "metadata": {
        "id": "9N0rcxR-797y",
        "outputId": "61f5bb0d-1486-4d56-8368-20d70730ca8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: minatar in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from minatar) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from minatar) (1.4.8)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from minatar) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from minatar) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from minatar) (2.2.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from minatar) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from minatar) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.11/dist-packages (from minatar) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from minatar) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from minatar) (0.13.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from minatar) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->minatar) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->minatar) (4.56.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->minatar) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->minatar) (11.1.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->minatar) (2025.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBMRNZF44KCg",
        "outputId": "45dad04d-18bb-489f-ee81-bfed68cf5d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch --upgrade\n",
        "!pip install triton --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6Pyx3IlovAVh",
        "outputId": "01e2b3a5-3bed-40ff-c6db-b6c0c6f0f58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neptune-client in /usr/local/lib/python3.11/dist-packages (1.13.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (3.1.44)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (11.1.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.11/dist-packages (from neptune-client) (2.10.1)\n",
            "Requirement already satisfied: boto3>=1.28.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (1.37.1)\n",
            "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (11.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (8.1.8)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (1.0.0)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from neptune-client) (24.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from neptune-client) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from neptune-client) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (2.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (1.17.0)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (2.3.0)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (1.8.0)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.28.0->neptune-client) (1.37.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.28.0->neptune-client) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.28.0->neptune-client) (0.11.2)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.1.1)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.6)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.0.2)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (3.20.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->neptune-client) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->neptune-client) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->neptune-client) (2025.1.31)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.23.0)\n",
            "Requirement already satisfied: importlib-resources>=1.3 in /usr/local/lib/python3.11/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->neptune-client) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->neptune-client) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->neptune-client) (2025.1)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.11/dist-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.22.3)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (3.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>0.1.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (0.1.1)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (24.11.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (2.9.0.20241206)\n"
          ]
        }
      ],
      "source": [
        "!pip install neptune-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abco6h-Wu-Dr"
      },
      "source": [
        "PyTorch device setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9ZahonRu-Dw"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk1jB5YCu-Dx"
      },
      "source": [
        "Seed for reproducible results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9ljETE3u-Dx"
      },
      "outputs": [],
      "source": [
        "seed = 2025\n",
        "np.random.seed(seed)\n",
        "np.random.default_rng(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZNUEIIgu-Dy"
      },
      "outputs": [],
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, input_shape, output_size):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        \n",
        "        c, h, w = input_shape  # Extract channels, height, width\n",
        "\n",
        "        self.conv1 = nn.Conv2d(c, 16, kernel_size=3, stride=1, padding=1)  # Conv layer\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) # Another Conv layer\n",
        "        self.fc1 = nn.Linear(32 * h * w, 128)  # Fully connected layer\n",
        "        self.fc2 = nn.Linear(128, output_size) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Permute the input from (batch_size, height, width, channels) to (batch_size, channels, height, width)\n",
        "        #need to do this to match shape expected for torch's Conv2d\n",
        "        x = x.permute(0, 3, 1, 2)  # Reorder dimensions to match PyTorch's Conv2d format: (batch_size, c, h, w)\n",
        "\n",
        "    #     x = F.relu(self.conv1(x))  # First Conv layer + ReLU\n",
        "    #     x = F.relu(self.conv2(x))  # Second Conv layer + ReLU\n",
        "    #     x = torch.flatten(x, start_dim=1)  # Flatten for the FC layer\n",
        "    #     x = F.relu(self.fc1(x))  # First FC layer\n",
        "    #     x = self.fc2(x)  # Output logits (no softmax, handled by policy function)\n",
        "    #     return F.softmax(x, dim=1)  # Action probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqAcRTB2u-Dz"
      },
      "outputs": [],
      "source": [
        "class ReinforceNNLearber:\n",
        "    def __init__(self, env, device, run, gamma=0.99, learning_rate=9.8e-4, num_episodes = 500):\n",
        "        self.env = env\n",
        "        self.run = run  # Store the Neptune run\n",
        "        self.gamma = gamma #discount factor\n",
        "        self.learning_rate = learning_rate\n",
        "        self.device = device\n",
        "        # Define the policy network\n",
        "\n",
        "        # Get number of actions from gym action space\n",
        "        ###THIS CHANGED FOR MINATAR\n",
        "        n_actions = self.env.num_actions()\n",
        "        # Reset and get the number of state observations\n",
        "        ###THIS CHANGED FOR MINATAR\n",
        "        self.env.reset()\n",
        "        state=self.env.state()\n",
        "\n",
        "\n",
        "        ###THIS CHANGED FOR MINATAR\n",
        "        # Assuming state shape is (10, 10, c), where c is the number of channels\n",
        "\n",
        "        self.policy_net = PolicyNet(state.shape, n_actions).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
        "        # when initialise like this, you're telling the optimizer how large each step (or update) should be in the gradient descent process. In this case, the learning rate (self.learning_rate) controls the size of the step taken during the optimization step.\n",
        "        self.num_episodes = num_episodes\n",
        "\n",
        "    def preprocess_state(self, state):\n",
        "        \"\"\"\n",
        "        Preprocesses the state by transforming it to tensor and adding a batch dimension.\n",
        "\n",
        "        Args:\n",
        "            state (numpy.ndarray): Input state from the environment.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: State tensor with batch dimension.\n",
        "        \"\"\"\n",
        "\n",
        "        #convert into a pytorch tensor. then unsqueeze to add dimension to the tensor at position zero - NN in pytorch expect batched inputs, first dim batch size.\n",
        "        #.to(self.device) moves the tensor to the device. Because operations on tensors in pytorch must happen on the same device.\n",
        "        normalized_state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
        "        normalized_state = normalized_state.unsqueeze(0)\n",
        "\n",
        "        return normalized_state\n",
        "\n",
        "\n",
        "    def compute_returns(self, rewards):\n",
        "        \"\"\"\n",
        "        Computes discounted returns.\n",
        "\n",
        "        Args:\n",
        "            rewards (list): List of rewards for each time step.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Array of discounted returns.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        t_steps = np.arange(len(rewards))\n",
        "        r = rewards * self.gamma ** t_steps # Compute discounted rewards for each time step\n",
        "\n",
        "        # Compute the discounted cumulative sum in reverse order and then reverse it again to restore the original order.\n",
        "        r = np.cumsum(r[::-1])[::-1] / self.gamma ** t_steps\n",
        "\n",
        "        return r\n",
        "    #What I had before was less efficient but achieved same result:\n",
        "    #returns = []\n",
        "    # cumulative_return = 0\n",
        "    # for _, _, reward, _ in reversed(episode):\n",
        "    #     cumulative_return = reward + self.gamma * cumulative_return\n",
        "    #     returns.insert(0, cumulative_return)\n",
        "\n",
        "\n",
        "    def compute_loss(self,log_probs, returns):\n",
        "        \"\"\"\n",
        "        Computes the policy gradient loss based on the formula.\n",
        "\n",
        "        Args:\n",
        "            log_probs (list): List of log probabilities of selected actions.\n",
        "            returns (numpy.ndarray): Array of discounted returns.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Computed loss.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        policy_loss = []\n",
        "        for log_prob, returns in zip(log_probs, returns):#pairs each element of the log_prob with the CORRESPONDING element in the returns list. Outputs an iterator of tuples each containing one element from log_prob and one element from returns.\n",
        "            #zip(epilog_probsode, returns) will create tuples like this: [(log_prob1, return_1),...]\n",
        "            policy_loss.append(-log_prob * returns)\n",
        "            # Negative sign for gradient ascent\n",
        "            #since we want to maximize the expected return but most optimizers minimize a loss function, we accumulate the negative term for each time step\n",
        "            #for each time step we calculate log_prob*Gt wehich represents how much the chosen action contributed to the overall reward in terms of the log-likelihood of the action multiplied by the expected reward (how much the action contributes to the final rewards, weighted by how likely the policy was to choose that action)\n",
        "\n",
        "        return torch.stack(policy_loss).sum()\n",
        "        #we sum all the losses for the entire episode\n",
        "        #torch.stack(policy_loss) takes the list of tensors stored in policy_loss and combines them into a single tensor, stacking them along a new dimension\n",
        "        #.sum() is applied to the stacked tensor to sum the individual elements of the loss across the batch\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        self.policy_net.train()  # Ensures the network is in training mode\n",
        "        episode_rewards = []\n",
        "        # Main training loop that loops over each episode\n",
        "        for episode in range(1, self.num_episodes + 1):\n",
        "            self.env.reset()\n",
        "            # state() returns a NumPy array of shape (10, 10, c)\n",
        "            state=self.env.state()\n",
        "\n",
        "            # Initialize empty lists to store log probabilities and rewards for the episode\n",
        "            log_probs = []\n",
        "            episode_reward = []\n",
        "\n",
        "            # Loop until the episode is done\n",
        "            while True:\n",
        "                #print(state.shape)\n",
        "                state = self.preprocess_state(state) # Preprocesses the state to convert it to tensor\n",
        "                #print(state.shape)\n",
        "                #print((state.permute(0, 3, 1, 2)).shape)\n",
        "                action_probs = self.policy_net(state) # Forward pass through policy network to get action probabilities\n",
        "                #Passes the current state through the policy network to comput the probabilities of taking each action.\n",
        "                #returns the action probabilities for the given state as predicted by the policy network. Type: pytorch tensor. Shape: (batch_size, num_actions)\n",
        "\n",
        "                # Sample an action from the action probabilities\n",
        "                dist = torch.distributions.Categorical(action_probs)\n",
        "                #creates a categorical (discrete - since discrete actions) probability distribution based on the action probabilities.\n",
        "                #The categorical distribution object provides methods to sample actions or comput the log_probabilities of actions\n",
        "                action = dist.sample()\n",
        "                #Take the action in the environment\n",
        "\n",
        "                # Compute the log probability of the sampled action\n",
        "                log_prob = dist.log_prob(action)\n",
        "                log_probs.append(log_prob)\n",
        "\n",
        "                # Take a step in the environment\n",
        "                reward, done = self.env.act(action.item())\n",
        "                next_state=self.env.state()\n",
        "                #same as before except now have .item() which extracts the scalar value from a pytorch tensor.\n",
        "                # Action is typically a PyTorch tensor representing the action selected by the policy. If action is sampled from a Categorical distribution, its type will be a 0-dimensional tensor (a scalar tensor), for example: tensor(2)\n",
        "                #.item() extracts the scalar value from the tensor and converts it into a plain python integer which is expected by gym as input to step()\n",
        "                episode_reward.append(reward)\n",
        "\n",
        "                state = next_state # Move to the next state\n",
        "\n",
        "                if done: # if the episode is done\n",
        "                    returns = self.compute_returns(episode_reward) # Compute the returns (discounted sum of rewards) for the episode\n",
        "                    loss = self.compute_loss(log_probs, returns)  # Compute the loss for the episode using the log probabilities and returns\n",
        "                    self.optimizer.zero_grad()\n",
        "                    #clears the gradients of all parameters (weights and biases) that are being optimized by the optimizer. It ensures that the gradients from the previous backward pass are reset to zero before the new backward pass. Ensures gradients are not accumulated.\n",
        "                    #In PyTorch, when you perform a forward pass through a neural network and then call .backward() to compute the gradients, the gradients of the parameters in the network are stored in the .grad attribute of each parameter.\n",
        "                    #These gradients are used for updating the parameters during the optimization step.\n",
        "                    #By default,  PyTorch accumulates gradients, meaning it adds the gradients computed during each backward pass to the existing ones. This is useful in some cases, such as when you are training with mini-batches and want to accumulate gradients across multiple steps before updating the parameters.\n",
        "                    #However, for each new training step, you typically want to reset the gradients to zero to avoid carrying over gradients from the previous batch or episode.\n",
        "\n",
        "                    loss.backward() #computes the gradients of the loss with respect to the parameters of the policy network using backpropagation.\n",
        "                    self.optimizer.step() #instance of an adam optimizer which updates the policy network's weights based on the gradients that were computed during backpropagation.\n",
        "                    #Adam optimizer automatically adjusts the learning rate for each parameter based on the gradients and their respective magnitudes.\n",
        "                    #The learning rate helps to control the step size in parameter updates, ensuring that the network learns at a reasonable pace without overshooting the optimal solution.\n",
        "\n",
        "                    episode_reward = sum(episode_reward) # Compute the total reward for the episode\n",
        "\n",
        "                    #print(f\"Episode {episode}, Ep Reward: {episode_reward}\")\n",
        "                    self.run[\"episode_rewards\"].log(episode_reward)\n",
        "                    episode_rewards.append(episode_reward)\n",
        "\n",
        "                    break # Exit the episode loop\n",
        "\n",
        "        return episode_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzNm3ujQ26ju"
      },
      "outputs": [],
      "source": [
        "def main_Minatar(learning_rate,gamma):\n",
        "    my_api=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZDg3ZjNlYi04MWI3LTQ1ODctOGIxNS1iNTY3ZjgzMGYzMzYifQ==\"\n",
        "    run = neptune.init_run(\n",
        "    project=\"EnergyGridRL/REINFORCE-Minatar\",\n",
        "    api_token=my_api\n",
        ")  # your credentials\n",
        "    # Create the environment\n",
        "    params = {\"GAMMA\":gamma,#0.99,\n",
        "               \"LR\":learning_rate,#9.8e-4\n",
        "               \"num_episodes\":10000}\n",
        "    run[\"parameters\"] = params# Use the grid size in the environment\n",
        "    env = Environment(env_name=\"breakout\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    # Create the Q-LEARNING agent\n",
        "\n",
        "    agent = ReinforceNNLearber(\n",
        "               env=env,\n",
        "               device=device,\n",
        "               run=run,\n",
        "               gamma=params[\"GAMMA\"],\n",
        "               learning_rate=params[\"LR\"],\n",
        "               num_episodes = params[\"num_episodes\"])\n",
        "\n",
        "    # Train the agent\n",
        "    episode_rewards = np.array(agent.train())\n",
        "\n",
        "    # Stop the Neptune run\n",
        "    run.stop()\n",
        "\n",
        "    plt.plot(episode_rewards)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"Training Performance\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXVpN_p280li",
        "outputId": "5326a507-4a3e-4ddb-bd50-5d7f244fb3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379451 sha256=089e93a859883606f22f5ef7a74bf5fb9758190ac10428b13f9ac930f1df4490\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "NlZbfAlY8Zk-",
        "outputId": "1f1b5974-469d-483f-cde2-3179b37e33b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/EnergyGridRL/LunarREINFORCE/e/LUN2-6\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "main_Minatar(1e-3,0.99)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}